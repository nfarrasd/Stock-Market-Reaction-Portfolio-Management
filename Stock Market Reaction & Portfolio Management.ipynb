{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Reaction & Portfolio Management\n",
    "**Author**: Moch Nabil Farras Dhiya\n",
    "\n",
    "**E-mail**: nabilfarras923@gmail.com\n",
    "\n",
    "**Institution**: Bandung Institute of Technology\n",
    "\n",
    "**Student ID**: 10120034\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**About**: This is a side-project created in order to analyze whether there is a relationship between COVID-19, Currency Exchange, and Stock Market, especially in Indonesia. The steps are of the followings:\n",
    "\n",
    "\n",
    "1.   Extracting data from External Sources (Yahoo Finance) and Local MySQL DB (soon changed to COVID-19 API)\n",
    "2.   Transform the data (making sure it is usable and consistent)\n",
    "3.   Analyze the data trend and perform hypothesis testing to check the relationship between the data\n",
    "4.   TSA forecasting to predict the stock price, using several models including, but not limited to ARIMA and Long Short-Term Memory\n",
    "5.   Portfolio Optimization using Markowitz Efficient Frontier model \n",
    "\n",
    "These steps will be automatically implemented on daily basis, so we only need to monitor the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with local machine\n",
    "import os\n",
    "\n",
    "# Extracting file from URL (Online website)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Extract stock market data\n",
    "import yfinance as yf\n",
    "\n",
    "# Importing and transforming file\n",
    "import pandas as pd\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import re # Cleaning texts\n",
    "import time\n",
    "import datetime as dt # Datetime manipulation\n",
    "from scipy.stats import pearsonr # Statistics\n",
    "from scipy.optimize import minimize # Minimze SR\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Time Series Analysis\n",
    "import itertools\n",
    "import pmdarima as pm # Automate ARIMA steps\n",
    "from statsmodels.api import qqplot # QQPlot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose # TSA Decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # Plot ACF & PACF Model\n",
    "from statsmodels.tsa.stattools import adfuller # Stationary data\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch # Lagrange Multiplier test\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX # Plot SARIMAX Model\n",
    "from statsmodels.tsa.api import Holt # Holt Model\n",
    "from arch import arch_model # GARCH Model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# CNN Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "# COVID19 Dataset\n",
    "from covid19dh import covid19\n",
    "\n",
    "# # Connect Colab with Local MySQL Database and delete existing table\n",
    "# import sqlalchemy as sql\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# # Notify is there is an error to email\n",
    "# import logging\n",
    "# import logging.handlers\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract COVID-19 Daily Cases Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already have a COVID-19 Daily Case data in our local database from previous projects, then we only need to fetch the data from our local database and make some modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c640\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\covid19dh\\main.py:107: UserWarning: error to fetch data\n",
      "  warnings.warn(\"error to fetch data\")\n"
     ]
    }
   ],
   "source": [
    "covid_data, src = covid19('Indonesia', verbose = False)\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'rename'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Kuliah\\Project\\Python\\TSA Indonesia Economic\\Stock Market Reaction & Portfolio Management.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Kuliah/Project/Python/TSA%20Indonesia%20Economic/Stock%20Market%20Reaction%20%26%20Portfolio%20Management.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m covid_data \u001b[39m=\u001b[39m covid_data\u001b[39m.\u001b[39;49mrename(columns \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdate_reported\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Kuliah/Project/Python/TSA%20Indonesia%20Economic/Stock%20Market%20Reaction%20%26%20Portfolio%20Management.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                           \u001b[39m'\u001b[39m\u001b[39mconfirmed\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mnew_cases\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Kuliah/Project/Python/TSA%20Indonesia%20Economic/Stock%20Market%20Reaction%20%26%20Portfolio%20Management.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m covid_data \u001b[39m=\u001b[39m covid_data[[\u001b[39m'\u001b[39m\u001b[39mdate_reported\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnew_cases\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mdate_reported\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Kuliah/Project/Python/TSA%20Indonesia%20Economic/Stock%20Market%20Reaction%20%26%20Portfolio%20Management.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m covid_data \u001b[39m=\u001b[39m (covid_data \u001b[39m-\u001b[39m covid_data\u001b[39m.\u001b[39mshift(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'rename'"
     ]
    }
   ],
   "source": [
    "covid_data = covid_data.rename(columns = {'date': 'date_reported',\n",
    "                                          'confirmed': 'new_cases'})\n",
    "covid_data = covid_data[['date_reported', 'new_cases']].set_index('date_reported')\n",
    "covid_data = (covid_data - covid_data.shift(1)).fillna(0)\n",
    "covid_data = covid_data[: '2023-02-28'].reset_index()\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MySQL 'COVID-19' Local DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"================================\")\n",
    "# print(\"Connecting to Local DB .....\")\n",
    "\n",
    "# # Credentials to database connection\n",
    "# hostname = \"localhost\"\n",
    "# dbname = \"covid_19\"\n",
    "# uname = os.getenv('DB_UNAME')\n",
    "# pwd = os.getenv('DB_PASSWORD')\n",
    "\n",
    "# # Create SQLAlchemy engine to connect to MySQL Database\n",
    "# sqlEngine = sql.create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\".format(host = hostname,\n",
    "#                                                                                db = dbname,\n",
    "#                                                                                user = uname,\n",
    "#                                                                                pw = pwd))\n",
    "\n",
    "# dbConnection = sqlEngine.connect()\n",
    "\n",
    "# print(\"Successfully to Local DB.\")\n",
    "# print(\"================================\")\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch COVID-19 Daily Case Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch only Indonesia data\n",
    "# sql = \"SELECT * FROM daily_case_death WHERE country = 'Indonesia'\"\n",
    "# covid_data = pd.read_sql(sql, \n",
    "#                          con = sqlEngine)\n",
    "# covid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it comes from our local DB (loaded from previous projects which involves ETL process), we can be sure that this dataframe is already clean and ready to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Currency Exchange Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the desired data from Yahoo Finance official website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start and End Date\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2023-02-28'\n",
    "# end_date = dt.datetime.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDR/USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDR/USD\n",
    "idr_usd_ticker = yf.Ticker(\"IDR%3DX\")\n",
    "    \n",
    "# get historical market data\n",
    "init_idr_usd = idr_usd_ticker.history(period = \"max\")\n",
    "init_idr_usd = init_idr_usd[start_date:]\n",
    "init_idr_usd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idr_usd = init_idr_usd.iloc[:, :4]\n",
    "init_idr_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDR/JPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDR/JPY\n",
    "idf_jpy_ticker = yf.Ticker(\"JPYIDR=X\")\n",
    "    \n",
    "# get historical market data\n",
    "init_idr_jpy = idf_jpy_ticker.history(period = \"max\")\n",
    "init_idr_jpy = init_idr_jpy[start_date:]\n",
    "init_idr_jpy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idr_jpy = init_idr_jpy.iloc[:, :4]\n",
    "init_idr_jpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Stock Market Price Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract 2 stock market prices (BBCA.JK and ^JKSE) using yfinance module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBCA.JK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_bbca_info = yf.download('BBCA.JK', \n",
    "                             start = start_date, \n",
    "                             end = end_date, \n",
    "                             progress = False)\n",
    "\n",
    "init_bbca_info = init_bbca_info.reset_index()\n",
    "init_bbca_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_bbca_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^JKSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_jkse_info = yf.download('^JKSE', \n",
    "                             start = start_date, \n",
    "                             end = end_date, \n",
    "                             progress = False)\n",
    "\n",
    "init_jkse_info = init_jkse_info.reset_index()\n",
    "init_jkse_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_jkse_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do 3 things:\n",
    "1. Expand and collapse the datasets so that it started from 2017-01-03\n",
    "2. Tidy up the column name format, i.e. replacing space with _ and use lower alphabetical format\n",
    "3. Change int datatype into float in stock price dataframe, and object into datetime in covid-19 dataframe\n",
    "4. Make sure there is no dirty data, i.e. negative values\n",
    "5. Make copy of the finance data, and merge it by interpolating. As we notice that there is always a 2 day gap every 5 day, possible happen because the data is not recorded in weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand and Collapse Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand COVID dataset\n",
    "covid_data_expanded = covid_data.append(pd.DataFrame({'date_reported': pd.date_range(start = '2017-01-03', \n",
    "                                                                                     end = '2020-01-21')}))\n",
    "covid_data_expanded = covid_data_expanded.sort_values(by = 'date_reported').fillna(0)\n",
    "\n",
    "# Collapse IDR/USD dataset\n",
    "init_idr_usd = init_idr_usd.iloc[1:].reset_index()\n",
    "\n",
    "# Collapse IDR/JPY dataset\n",
    "init_idr_jpy = init_idr_jpy.iloc[1:].reset_index()\n",
    "\n",
    "# Collapse BBCA.JK dataset\n",
    "init_bbca_info = init_bbca_info.iloc[1:].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the column names to sql readable format\n",
    "def remove_space(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace(' ', '_')\n",
    "    return text\n",
    "\n",
    "# Notice that all of the currency exchange and stock prices dataframe above have the same format\n",
    "# so, we can tidy them all at once by looping\n",
    "for df in [init_idr_usd, init_idr_jpy, init_bbca_info, init_jkse_info]:\n",
    "    temp_target = []\n",
    "    for col in df.columns:\n",
    "        temp_target.append(remove_space(col.lower()))\n",
    "        \n",
    "    temp_dct = dict(zip(df.columns, temp_target))\n",
    "    df = df.rename(columns = temp_dct, \n",
    "                   inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock market price dataframe\n",
    "for df in [init_idr_usd, init_idr_jpy, init_bbca_info, init_jkse_info]:\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes == 'int64':\n",
    "            df[col] = df[col].astype('float')\n",
    "            \n",
    "\n",
    "# COVID-19 dataframe\n",
    "covid_data['date_reported'] = covid_data['date_reported'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Value(s) Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any negative values into positive, if there is any\n",
    "for df in [init_idr_usd, init_idr_jpy, init_bbca_info, init_jkse_info]:\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtypes == 'int64') or (df[col].dtypes == 'float64'):\n",
    "            df[col] = df[col].apply(lambda x: x if x >= 0 else -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the Finance Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idr_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDR/USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "idr_usd = pd.DataFrame()\n",
    "\n",
    "idr_usd['date'] = pd.date_range(start = '2017-01-03', \n",
    "                                end = end_date)\n",
    "\n",
    "# Merge the data\n",
    "idr_usd = pd.merge(idr_usd, init_idr_usd, how = 'left', on = 'date')\n",
    "idr_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDR/JPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "idr_jpy = pd.DataFrame()\n",
    "\n",
    "idr_jpy['date'] = pd.date_range(start = '2017-01-03',\n",
    "                                end = end_date)\n",
    "\n",
    "# Merge the data\n",
    "idr_jpy = pd.merge(idr_jpy, init_idr_jpy, how = 'left', on = 'date')\n",
    "idr_jpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBCA.JK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "bbca_info = pd.DataFrame()\n",
    "\n",
    "bbca_info['date'] = pd.date_range(start = '2017-01-03', \n",
    "                                  end = end_date)\n",
    "\n",
    "# Merge the data\n",
    "bbca_info = pd.merge(bbca_info, init_bbca_info, how = 'left', on = 'date')\n",
    "bbca_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^JKSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "jkse_info = pd.DataFrame()\n",
    "\n",
    "jkse_info['date'] = pd.date_range(start = '2017-01-03', \n",
    "                                  end = end_date)\n",
    "\n",
    "# Merge the data\n",
    "jkse_info = pd.merge(jkse_info, init_jkse_info, how = 'left', on = 'date')\n",
    "jkse_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value(s) Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original COVID Dataset\n",
    "\n",
    "missing_col = pd.DataFrame()\n",
    "col_names = []\n",
    "missing_value_percentage = []\n",
    "\n",
    "for col in covid_data.columns:\n",
    "    percentage = 100*len(covid_data.loc[(covid_data[col].isna()) \n",
    "                                        | (covid_data[col] == '-')])/len(covid_data)\n",
    "\n",
    "    if percentage != 0:\n",
    "        col_names.append(col)\n",
    "        missing_value_percentage.append(percentage)\n",
    "        \n",
    "missing_col['col_names'] = col_names\n",
    "missing_col['missing_value_percentage'] = missing_value_percentage\n",
    "\n",
    "missing_col = missing_col.sort_values(by = 'missing_value_percentage').reset_index(drop = True)\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded COVID Dataset\n",
    "\n",
    "missing_col = pd.DataFrame()\n",
    "col_names = []\n",
    "missing_value_percentage = []\n",
    "\n",
    "for col in covid_data.columns:\n",
    "    percentage = 100 * len(covid_data_expanded.loc[(covid_data_expanded[col].isna()) \n",
    "                                                 | (covid_data_expanded[col] == '-')])/len(covid_data_expanded)\n",
    "\n",
    "    if percentage != 0:\n",
    "        col_names.append(col)\n",
    "        missing_value_percentage.append(percentage)\n",
    "        \n",
    "missing_col['col_names'] = col_names\n",
    "missing_col['missing_value_percentage'] = missing_value_percentage\n",
    "\n",
    "missing_col = missing_col.sort_values(by = 'missing_value_percentage').reset_index(drop = True)\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currency Exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDR/USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = pd.DataFrame()\n",
    "col_names = []\n",
    "missing_value_percentage = []\n",
    "\n",
    "for col in idr_usd.columns:\n",
    "    percentage = 100*len(idr_usd.loc[(idr_usd[col].isna()) \n",
    "                                     | (idr_usd[col] == '-')])/len(idr_usd)\n",
    "\n",
    "    if percentage != 0:\n",
    "        col_names.append(col)\n",
    "        missing_value_percentage.append(percentage)\n",
    "        \n",
    "missing_col['col_names'] = col_names\n",
    "missing_col['missing_value_percentage'] = missing_value_percentage\n",
    "\n",
    "missing_col = missing_col.sort_values(by = 'missing_value_percentage').reset_index(drop = True)\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDR/JPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = pd.DataFrame()\n",
    "col_names = []\n",
    "missing_value_percentage = []\n",
    "\n",
    "for col in idr_jpy.columns:\n",
    "    percentage = 100*len(idr_jpy.loc[(idr_jpy[col].isna()) \n",
    "                                     | (idr_jpy[col] == '-')])/len(idr_jpy)\n",
    "\n",
    "    if percentage != 0:\n",
    "        col_names.append(col)\n",
    "        missing_value_percentage.append(percentage)\n",
    "        \n",
    "missing_col['col_names'] = col_names\n",
    "missing_col['missing_value_percentage'] = missing_value_percentage\n",
    "\n",
    "missing_col = missing_col.sort_values(by = 'missing_value_percentage').reset_index(drop = True)\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Market Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBCA.JK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = pd.DataFrame()\n",
    "col_names = []\n",
    "missing_value_percentage = []\n",
    "\n",
    "for col in bbca_info.columns:\n",
    "    percentage = 100*len(bbca_info.loc[(bbca_info[col].isna()) \n",
    "                                       | (bbca_info[col] == '-')])/len(bbca_info)\n",
    "\n",
    "    if percentage != 0:\n",
    "        col_names.append(col)\n",
    "        missing_value_percentage.append(percentage)\n",
    "        \n",
    "missing_col['col_names'] = col_names\n",
    "missing_col['missing_value_percentage'] = missing_value_percentage\n",
    "\n",
    "missing_col = missing_col.sort_values(by = 'missing_value_percentage').reset_index(drop = True)\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^JKSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = pd.DataFrame()\n",
    "col_names = []\n",
    "missing_value_percentage = []\n",
    "\n",
    "for col in jkse_info.columns:\n",
    "    percentage = 100*len(jkse_info.loc[(jkse_info[col].isna()) \n",
    "                                       | (jkse_info[col] == '-')])/len(jkse_info)\n",
    "\n",
    "    if percentage != 0:\n",
    "        col_names.append(col)\n",
    "        missing_value_percentage.append(percentage)\n",
    "        \n",
    "missing_col['col_names'] = col_names\n",
    "missing_col['missing_value_percentage'] = missing_value_percentage\n",
    "\n",
    "missing_col = missing_col.sort_values(by = 'missing_value_percentage').reset_index(drop = True)\n",
    "missing_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are null values in these datasets and that the missing value percentage is quite high (>25%) due to the inactiveness in the weekends. Even so, we will try to do imputation on the characteristics of the data as we want to analyze it.\n",
    "\n",
    "Since the data is a **TS numerical data**, then we will do **mean** based on the **nearest non-null value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDR/USD dataset\n",
    "a = idr_usd.rolling(3).mean()\n",
    "b = idr_usd.iloc[::-1].rolling(3).mean()\n",
    "\n",
    "c = a.fillna(b).fillna(idr_usd).interpolate(method = 'nearest').ffill().bfill()\n",
    "\n",
    "idr_usd = idr_usd.fillna(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDR/JPY dataset\n",
    "a = idr_jpy.rolling(3).mean()\n",
    "b = idr_jpy.iloc[::-1].rolling(3).mean()\n",
    "\n",
    "c = a.fillna(b).fillna(idr_jpy).interpolate(method = 'nearest').ffill().bfill()\n",
    "\n",
    "idr_jpy = idr_jpy.fillna(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBCA.JK dataset\n",
    "a = bbca_info.rolling(3).mean()\n",
    "b = bbca_info.iloc[::-1].rolling(3).mean()\n",
    "\n",
    "c = a.fillna(b).fillna(bbca_info).interpolate(method = 'nearest').ffill().bfill()\n",
    "\n",
    "bbca_info = bbca_info.fillna(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^JKSE dataset\n",
    "a = jkse_info.rolling(3).mean()\n",
    "b = jkse_info.iloc[::-1].rolling(3).mean()\n",
    "\n",
    "c = a.fillna(b).fillna(jkse_info).interpolate(method = 'nearest').ffill().bfill()\n",
    "\n",
    "jkse_info = jkse_info.fillna(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Daily Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we only interested in the daily cases of COVID-19, we can drop the rest of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 daily cases plot\n",
    "fig, ax = plt.subplots(constrained_layout = True)\n",
    "covid_data_expanded.set_index('date_reported').plot(ax = ax, \n",
    "                                                    figsize = (18,12))\n",
    "\n",
    "# First date COVID discovery in Indonesia\n",
    "ax.axvline(x = \"2020-03-02\", \n",
    "           color = 'blue', \n",
    "           linestyle = '--')\n",
    "\n",
    "# 2020 Eid Al-Fitr\n",
    "ax.axvline(x = \"2020-05-23\", \n",
    "           color = 'purple', \n",
    "           linestyle = '--') \n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax.axvline(x = \"2020-08-17\", \n",
    "           color = 'green', \n",
    "           linestyle = '--') \n",
    "\n",
    "# 2021 New Year\n",
    "ax.axvline(x = \"2021-01-01\", \n",
    "           color = 'brown', \n",
    "           linestyle = '--')\n",
    "\n",
    "# 2021 Eid Al-Fitr\n",
    "ax.axvline(x = \"2021-05-12\", \n",
    "           color = 'purple', \n",
    "           linestyle = '--') \n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax.axvline(x = \"2021-08-17\", \n",
    "           color = 'green', \n",
    "           linestyle = '--') \n",
    "\n",
    "# 2022 New Year\n",
    "ax.axvline(x = \"2022-01-01\", \n",
    "           color = 'brown', \n",
    "           linestyle = '--') \n",
    "\n",
    "ax.set_ylabel(\"Daily cases\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title('COVID-19 New Cases in Indonesia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 daily cases plot\n",
    "fig, ax = plt.subplots(constrained_layout = True)\n",
    "covid_data_expanded.set_index('date_reported')[\"2020-03-02\":].plot(ax = ax, \n",
    "                                                                   figsize = (18,12))\n",
    "\n",
    "# First date COVID discovery in Indonesia\n",
    "ax.axvline(x = \"2020-03-02\", \n",
    "           color = 'blue', \n",
    "           linestyle = '--')\n",
    "\n",
    "# 2020 Eid Al-Fitr\n",
    "ax.axvline(x = \"2020-05-23\", \n",
    "           color = 'purple', \n",
    "           linestyle = '--') \n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax.axvline(x = \"2020-08-17\", \n",
    "           color = 'green', \n",
    "           linestyle = '--') \n",
    "\n",
    "# 2021 New Year\n",
    "ax.axvline(x = \"2021-01-01\", \n",
    "           color = 'brown', \n",
    "           linestyle = '--')\n",
    "\n",
    "# 2021 Eid Al-Fitr\n",
    "ax.axvline(x = \"2021-05-12\", \n",
    "           color = 'purple', \n",
    "           linestyle = '--') \n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax.axvline(x = \"2021-08-17\", \n",
    "           color = 'green', \n",
    "           linestyle = '--') \n",
    "\n",
    "# 2022 New Year\n",
    "ax.axvline(x = \"2022-01-01\", \n",
    "           color = 'brown', \n",
    "           linestyle = '--') \n",
    "\n",
    "ax.set_ylabel(\"Daily cases\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title('COVID-19 New Cases in Indonesia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above, we can see that:\n",
    "1. There is a significant **increment** on daily cases, especially **2-4 weeks after New Year's Holiday**, because people's mobility during holiday is very high, and will most likely spread the virus.\n",
    "2. Significant **increment** trend is also found not long **after Eid Fitri Holiday**, since the majority of Indonesian are Muslim, and usually people go back to their hometown on such occassion, making certain locations are crowded, and the virus will most likely to spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currency Exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we will use the 'Close' feature, as\n",
    "\n",
    "1. 'Close' price reflects all of the information available to all market participants (especially institutional market participants who have more accurate information) at the end of trading the stock.\n",
    "2. (especially for hedge funds or mutual fund managers) the close price is a determinant of the performance and wealth of investors for the day.\n",
    "3. 'Close' price reflects the price position at which the investor dares to hold a position, in the face of all information that may occur at night, when there is no trade.\n",
    "4. More than 90% of technical indicators used by technical analysts use close prices as their main input. This causes the position of the price to close, can trigger a buy signal or a sell signal.\n",
    "\n",
    "Source : https://sahamology.id/arti-harga-open-harga-high-harga-low-dan-harga-close-dalam-analisis-teknikal/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDR/USD\n",
    "idr_usd = idr_usd[['date', 'close']]\n",
    "\n",
    "# IDR/JPY\n",
    "idr_jpy = idr_jpy[['date', 'close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indonesia's Currency Exchange plot\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, constrained_layout = True)\n",
    "idr_usd.set_index('date').plot(ax = ax1, \n",
    "                               figsize = (18,12))\n",
    "\n",
    "idr_jpy.set_index('date').plot(ax = ax2, \n",
    "                               figsize = (18,12))\n",
    "\n",
    "# First date COVID discovery\n",
    "ax1.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--') # in Indonesia\n",
    "\n",
    "ax2.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--') # in Indonesia\n",
    "\n",
    "ax1.axvline(x = \"2020-01-20\", \n",
    "            color = 'black', \n",
    "            linestyle = '--') # in USA\n",
    "\n",
    "ax2.axvline(x = \"2020-01-16\", \n",
    "            color = 'purple', \n",
    "            linestyle = '--') # in Japan\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2020-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2020-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--')\n",
    "\n",
    "# 2021 New Year\n",
    "ax1.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2021-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--')\n",
    "\n",
    "# 2022 New Year\n",
    "ax1.axvline(x = \"2022-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2022-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "ax1.set_ylabel(\"Close\")\n",
    "ax2.set_ylabel(\"Close\")\n",
    "\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "\n",
    "ax1.set_title('IDR/USD Curency Exchange 2017 - 2022')\n",
    "ax2.set_title('IDR/JPY Currency Exchange 2017 - 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are outliers in the data, but we will **let them** as it is in this analysis. Since it is probably happen because of an **error in the system**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post COVID-19 Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indonesia's Currency Exchange plot\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, constrained_layout = True)\n",
    "idr_usd.set_index('date')[\"2020-01-01\":].plot(ax = ax1, \n",
    "                                              figsize = (18,12))\n",
    "\n",
    "idr_jpy.set_index('date')[\"2020-01-01\":].plot(ax = ax2, \n",
    "                                              figsize = (18,12))\n",
    "\n",
    "# First date COVID discovery\n",
    "ax1.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--') # in Indonesia\n",
    "\n",
    "ax2.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--') # in Indonesia\n",
    "\n",
    "ax1.axvline(x = \"2020-01-20\", \n",
    "            color = 'black', \n",
    "            linestyle = '--') # in USA\n",
    "\n",
    "ax2.axvline(x = \"2020-01-16\", \n",
    "            color = 'purple', \n",
    "            linestyle = '--') # in Japan\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2020-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2020-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--')\n",
    "\n",
    "# 2021 New Year\n",
    "ax1.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2021-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--')\n",
    "\n",
    "# 2022 New Year\n",
    "ax1.axvline(x = \"2022-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2022-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "ax1.set_ylabel(\"Close\")\n",
    "ax2.set_ylabel(\"Close\")\n",
    "\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "\n",
    "ax1.set_title('IDR/USD Curency Exchange 2017 - 2022')\n",
    "ax2.set_title('IDR/JPY Currency Exchange 2017 - 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the same argument as the 'Currency Exchange' section, we will also use the 'Close' feature in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBCA.JK\n",
    "bbca_info = bbca_info[['date', 'close']]\n",
    "\n",
    "# CMPP.JK\n",
    "jkse_info = jkse_info[['date', 'close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock market price plot\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, constrained_layout = True)\n",
    "bbca_info.set_index('date').plot(ax = ax1, \n",
    "                                 figsize = (18,12))\n",
    "\n",
    "jkse_info.set_index('date').plot(ax = ax2, \n",
    "                                 figsize = (18,12))\n",
    "\n",
    "# First date COVID discovery in Indonesia\n",
    "ax1.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--')\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2020-08-17\", \n",
    "            color = 'green',\n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2020-08-17\",\n",
    "            color = 'green', \n",
    "            linestyle = '--')\n",
    "\n",
    "# 2021 New Year\n",
    "ax1.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2021-08-17\",\n",
    "            color = 'green', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-08-17\", \n",
    "            color = 'green',\n",
    "            linestyle = '--')\n",
    "\n",
    "# 2022 New Year\n",
    "ax1.axvline(x = \"2022-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2022-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "ax1.set_ylabel(\"Close\")\n",
    "ax2.set_ylabel(\"Close\")\n",
    "\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "\n",
    "ax1.set_title('BBCA.JK Stock Price 2017 - 2022')\n",
    "ax2.set_title('^JKSE Stock Price 2017 - 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post COVID-19 Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock market price post COVID plot\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, constrained_layout = True)\n",
    "bbca_info.set_index('date')[\"2020-03-02\":].plot(ax = ax1, \n",
    "                                                figsize = (18,12))\n",
    "\n",
    "jkse_info.set_index('date')[\"2020-03-02\":].plot(ax = ax2, \n",
    "                                                figsize = (18,12))\n",
    "\n",
    "# First date COVID discovery in Indonesia\n",
    "ax1.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2020-03-02\", \n",
    "            color = 'blue', \n",
    "            linestyle = '--')\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2020-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2020-08-17\", \n",
    "            color = 'green', \n",
    "            linestyle = '--')\n",
    "\n",
    "# 2021 New Year\n",
    "ax1.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-01-01\", \n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "# Indonesia's Independence Day\n",
    "ax1.axvline(x = \"2021-08-17\",\n",
    "            color = 'green', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2021-08-17\", \n",
    "            color = 'green',\n",
    "            linestyle = '--')\n",
    "\n",
    "# 2022 New Year\n",
    "ax1.axvline(x = \"2022-01-01\",\n",
    "            color = 'brown', \n",
    "            linestyle = '--') \n",
    "\n",
    "ax2.axvline(x = \"2022-01-01\",\n",
    "            color = 'brown', \n",
    "            linestyle = '--')\n",
    "\n",
    "ax1.set_ylabel(\"Close\")\n",
    "ax2.set_ylabel(\"Close\")\n",
    "\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "\n",
    "ax1.set_title('BBCA.JK Stock Price post COVID-19 Discovery in Indonesia')\n",
    "ax2.set_title('^JKSE Stock Price post COVID-19 Discovery in Indonesia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above, we can see that:\n",
    "1. On the **first 3-4 weeks of COVID-19 discovery in Indonesia**, the price of BBCA.JK and ^JKSE stock market went **down** for **31.21%** and **30.31%**, respecively, and managed to recover since the first two weeks of April 2020.\n",
    "2. **2-3 weeks after Indonesia's Independence day in 2020**, there was also a **decremental** trend for both stock market price, and it went down for **8.65%** and **7.62%** , respectively. But these stock prices recovered since October 2020.\n",
    "3. Not long after **2021' New Year's Eve**, there is also a **decremental** trend for both stock market price, and it went down for **15.37%** and **3.42%**, respectively, and did not show any sign of recovery until October 2021.\n",
    "4. Surprisingly, both stock market prices give an **incremental** trend starting from **October in 2020 and 2021**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see if any pair of these datasets correlate with each other by using hypothesis testing with 0.05 significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are datasets which start from 2017-01-02, and there is also datasets which start from 2017-01-03. For consistency, we will drop the data on 2017-01-02 and start the index from 2017-01-03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr_usd = idr_usd.set_index('date')['2017-01-03':]\n",
    "\n",
    "idr_jpy = idr_jpy.set_index('date')['2017-01-03':]\n",
    "\n",
    "bbca_info = bbca_info.set_index('date')['2017-01-03':]\n",
    "\n",
    "jkse_info = jkse_info.set_index('date')['2017-01-03':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df.index = idr_usd.index\n",
    "df['idr_usd'] = idr_usd.values\n",
    "df['idr_jpy'] = idr_jpy.values\n",
    "df['bbca_info'] = bbca_info.values\n",
    "df['jkse_info'] = jkse_info.values\n",
    "\n",
    "significance_level = 0.05\n",
    "print('Significance level = 0.05 \\n')\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "features = []\n",
    "correlations = []\n",
    "p_values = []\n",
    "conclusions = []\n",
    "for col in df.columns:\n",
    "    for sub_col in df.columns:\n",
    "        if col != sub_col:\n",
    "            r, p = pearsonr(df.dropna()[col], df.dropna()[sub_col])\n",
    "            \n",
    "            features.append(f'{col} vs. {sub_col}')\n",
    "            correlations.append(r)\n",
    "            p_values.append(p)\n",
    "            \n",
    "            if p < significance_level:\n",
    "                conclusions.append('Reject H0')\n",
    "                \n",
    "            else:\n",
    "                conclusions.append('Do not reject H0')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "temp['features'] = features\n",
    "temp['correlations'] = correlations\n",
    "temp['p_values'] = p_values\n",
    "temp['conclusions'] = conclusions\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the hypothesis testing result, we can conclude that **IDR/USD** **moderately correlates** with both BBCA dataset, with pearson coefficient of 0.624731. In addition, BBCA dataset also **moderately correlates** with both ^JKSE dataset, with pearson coefficient of 0.612116, while the other pair datasets do not correlate with each other, using significance level of 0.05.\n",
    "\n",
    "**Note**: IDR/USD and ^JKSE datasets do correlate with each other as its p-value is significant, but put in mind that its pearson coefficient is **close to 0**. And such, we will **treat** it as they **do not correlate** with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vs. COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data_expanded = covid_data_expanded.set_index('date_reported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr_usd_covid = idr_usd['2020-01-03':covid_data_expanded.index[-1].strftime('%Y-%m-%d')]\n",
    "idr_usd_covid = idr_usd_covid.reset_index()\n",
    "\n",
    "idr_jpy_covid = idr_jpy['2020-01-03':covid_data_expanded.index[-1].strftime('%Y-%m-%d')]\n",
    "idr_jpy_covid = idr_jpy_covid.reset_index()\n",
    "\n",
    "bbca_info_covid = bbca_info['2020-01-03':covid_data_expanded.index[-1].strftime('%Y-%m-%d')]\n",
    "bbca_info_covid = bbca_info_covid.reset_index()\n",
    "\n",
    "jkse_info_covid = jkse_info['2020-01-03':covid_data_expanded.index[-1].strftime('%Y-%m-%d')]\n",
    "jkse_info_covid = jkse_info_covid.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr_usd_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df.index = covid_data_expanded['2020-01-03':].index\n",
    "df['covid'] = covid_data_expanded['2020-01-03':]['new_cases'].values\n",
    "df['idr_usd'] = idr_usd_covid['close'].values\n",
    "df['idr_jpy'] = idr_jpy_covid['close'].values\n",
    "df['bbca_info'] = bbca_info_covid['close'].values\n",
    "df['jkse_info'] = jkse_info_covid['close'].values\n",
    "\n",
    "significance_level = 0.05\n",
    "print('Significance level = 0.05 \\n')\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "features = []\n",
    "correlations = []\n",
    "p_values = []\n",
    "conclusions = []\n",
    "for col in df.columns:\n",
    "    for sub_col in df.columns:\n",
    "        if col != sub_col:\n",
    "            r, p = pearsonr(df.dropna()[col], df.dropna()[sub_col])\n",
    "            \n",
    "            features.append(f'{col} vs. {sub_col}')\n",
    "            correlations.append(r)\n",
    "            p_values.append(p)\n",
    "            \n",
    "            if p < significance_level:\n",
    "                conclusions.append('Reject H0')\n",
    "                \n",
    "            else:\n",
    "                conclusions.append('Do not reject H0')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "temp['features'] = features\n",
    "temp['correlations'] = correlations\n",
    "temp['p_values'] = p_values\n",
    "temp['conclusions'] = conclusions\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, for simplicity purposes, we will only focus on performing TSA on the BBCA data. If you want to perform TSA on another data, then just repeat the process and apply it to the data you want to perform TSA.\n",
    "\n",
    "In particular, these dataset is chosen since it is a financial data, so it has high volatility, and one of our aim here is to compare base model with ARCH/GARCH model. In addition, from the previous section, we know that BBCA dataset moderately/strongly correlate with other datasets, and we will use ARIMAX model with the other dataset as the exogenous arguments to predict the BBCA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform TSA, the data itself **has to be a stationary data** as its statistical properties do not change over time. The assumption in time series data modeling is that each observation is independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbca_info = bbca_info['2020-04-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,12))\n",
    "\n",
    "additive_result = seasonal_decompose(bbca_info.values, model = 'additive', period = 1)\n",
    "additive_result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,12))\n",
    "\n",
    "multiplicative_result = seasonal_decompose(bbca_info.values, model = 'multiplicative', period = 1)\n",
    "multiplicative_result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we had observe from the the graph above that our dataset has **increment trend** (additive, in particular, as the multiplicative residual are high. But we will see this in further section). Thus, it **should be not stationer**. We will check this claim by **ADF-Test using significance level 95%** ($\\alpha$ = 0.05) and from its **ACF/PACF Plot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test = adfuller(bbca_info['close'])\n",
    "\n",
    "# Statistic result\n",
    "print('Augmented-Dickey Fuller Result :', test[0])\n",
    "\n",
    "# p-value\n",
    "print('p-value :', test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, \n",
    "                               figsize = (18,12))\n",
    " \n",
    "# ACF Plot from stock market price data\n",
    "plot_acf(bbca_info, \n",
    "         lags = 15, \n",
    "         zero = False,\n",
    "         ax = ax1)\n",
    "\n",
    "# PACF Plot from stock market price data\n",
    "plot_pacf(bbca_info, \n",
    "          lags = 15, \n",
    "          zero = False, \n",
    "          ax = ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the hypothesis testing result above, we can conclude that the BBCA dataset is **not stationary**, as its **p-value** is **higher** than the $\\alpha$. And then, also notice that it has **no significant** change in the **ACF** plot. So, we will perform **differentiation** on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into Stationer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not stationary, it is necessary to transform the data. One of the transformation methods is to use the **differentiation method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiation\n",
    "bbca_info_diff = bbca_info.diff().dropna()\n",
    "\n",
    "# Differentiation result plotting\n",
    "bbca_info_diff.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adfuller Test\n",
    "test_1 = adfuller(bbca_info_diff['close'])\n",
    "\n",
    "# Statistic result\n",
    "print('Augmented-Dickey Fuller Result :', test_1[0])\n",
    "\n",
    "# p-value\n",
    "print('p-value :', test_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is already lower than the $\\alpha$, then it means that the BBCA dataset is now stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide the order of p and q in the ARIMA model, i.e. estimating the most suitable model for the data, there are 2 steps that we can do:\n",
    "\n",
    "1. **Manual**: Analyze the ACF and PACF plot and look at which lag the cut off happen to determine the p and q then look for the least AIC and BIC model.\n",
    "\n",
    "2. **Auto**: Use automated iterative steps to look for the model with the least AIC for the given range of p and q using pmdarima package.\n",
    "\n",
    "For simplicity, here we will only use the **Auto method** and will only look for the least **AIC** model. Here, AIC criteria is chosen because in financial datasets, the volatility is high, and considering the **unpredicted external factors** (e.g. COVID-19), there will **not** be a **fixed/final model** in such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation Method (Auto-ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best ARIMA model with lowest AIC\n",
    "results_auto_arima_bbca = pm.auto_arima(bbca_info, \n",
    "                                        start_p = 0,\n",
    "                                        start_q = 0,\n",
    "                                        max_p = 10,\n",
    "                                        max_q = 10,\n",
    "                                        d = None,\n",
    "                                        trace = True,\n",
    "                                        error_action = 'ignore')\n",
    "\n",
    "results_auto_arima_bbca.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary table above, we can see that the **ARIMA(3,1,1)** and **ARIMA(4,1,0)** have the **lowest AICs**. Thus, we will try to analyze the performance of that model by using **diagnostic test**. If it is not suited to be a predictor model, we will pick another model (iteratively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA(3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1a_bbca = SARIMAX(bbca_info, \n",
    "                        order = (3,1,1))\n",
    "\n",
    "results_1a_bbca = model_1a_bbca.fit()\n",
    "\n",
    "results_1a_bbca.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "----\n",
    "\n",
    "Notice that even without doing diagnostic test (i.e. Ljung Box Test), we can already see that the **parameters** of this model are **not statistically significant**. Thus, we will not use this model for our forecasting task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA(4,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1b_bbca = SARIMAX(bbca_info, \n",
    "                        order = (4,1,0))\n",
    "\n",
    "results_1b_bbca = model_1b_bbca.fit()\n",
    "\n",
    "results_1b_bbca.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_resid = results_1b_bbca.resid\n",
    "white_noise_arima = acorr_ljungbox(arima_resid ** 2, lags = [10], return_df = True)\n",
    "white_noise_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnostic Test & Conclusion**\n",
    "\n",
    "**Ljung-Box**\n",
    "\n",
    "----\n",
    "\n",
    "$H_0$ = Error term is white noise\n",
    "\n",
    "$H_1$ = Error term is not white noise\n",
    "\n",
    "Based on the table above, get that the p-value for Ljung-Box statistical test is 0.01. Since p-value = 0.01 < 0.05 = $\\alpha$, we **reject our null hypothesis**. Thus, we conclude that our **residuals** are **not independently distributed** using significance level of 0.05. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "----\n",
    "\n",
    "Based on the hypothesis testing and the model summary above, we can conclude that the residual from our ARIMA(4,1,0) is **not white noise**, based on significance level of 0.05. Thus, we **will** conduct GARCH to fit our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model = SARIMAX(bbca_info, \n",
    "                      order = (4,1,0)).fit()\n",
    "\n",
    "arima_resid = arima_model.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMAX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, since we know that BBCA datasets moderately/strongly correlate with other datasets, we will try to use the other as the exogenous arguments to predict our BBCA dataset. In this case, we will use **only IDR/USD** dataset as our **exogenous argument**. JKSE dataset is not included since both are stock market dataset, and it is not a wise choice to use dataset with the same 'type' as the exogenous argument. In the other hand, COVID dataset is not used since it has different starting point, and if we want to use it as our exogenous argument, we have to cut our BBCA dataset, which will cause a huge loss information, not considering that COVID is only temporary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch other datasets and 'merge' it into BBCA dataset\n",
    "exog_idr_usd = pd.DataFrame()\n",
    "exog_idr_usd.index = bbca_info.index\n",
    "exog_idr_usd['close'] = idr_usd['2020-04-01':]['close']\n",
    "\n",
    "exog_idr_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best ARIMAX model with lowest AIC\n",
    "results_auto_arimax_bbca = pm.auto_arima(bbca_info, \n",
    "                                         start_p = 0,\n",
    "                                         start_q = 0,\n",
    "                                         max_p = 10,\n",
    "                                         max_q = 10,\n",
    "                                         d = None,\n",
    "                                         exog = exog_idr_usd.values,\n",
    "                                         trace = True,\n",
    "                                         error_action = 'ignore')\n",
    "\n",
    "results_auto_arimax_bbca.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary table above, we can see that as the previous ones, the **ARIMAX(3,1,1,'IDR/USD')** and **ARIMAX(4,1,0,'IDR/USD')** have the **lowest AICs**. Thus, we will try to analyze the performance of that model by using **diagnostic test**. If it is not suited to be a predictor model, we will pick another model (iteratively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMAX(3,1,1,'IDR/USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ARIMAX(2,1,2,'IDR/USD')\n",
    "model_2a_bbca = SARIMAX(bbca_info, \n",
    "                        order = (3,1,1),\n",
    "                        exog = exog_idr_usd.values)\n",
    "\n",
    "results_2a_bbca = model_2a_bbca.fit()\n",
    "\n",
    "results_2a_bbca.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "----\n",
    "\n",
    "Notice that even without doing diagnostic test (i.e. Ljung-Box Test), we can already see that the **parameters** of this model are **not statistically significant**. Thus, we will not use this model for our forecasting task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMAX(4,1,0,'IDR/USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ARIMAX(4,1,0,'IDR/USD')\n",
    "model_2b_bbca = SARIMAX(bbca_info, \n",
    "                       order = (4,1,0),\n",
    "                       exog = exog_idr_usd.values)\n",
    "\n",
    "results_2b_bbca = model_2b_bbca.fit()\n",
    "\n",
    "results_2b_bbca.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arimax_resid = results_2b_bbca.resid\n",
    "white_noise_arimax = acorr_ljungbox(arimax_resid ** 2, lags = [10], return_df = True)\n",
    "white_noise_arimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnostic Test & Conclusion**\n",
    "\n",
    "**Ljung-Box**\n",
    "\n",
    "----\n",
    "\n",
    "$H_0$ = Error term is white noise\n",
    "\n",
    "$H_1$ = Error term is not white noise\n",
    "\n",
    "Based on the table above, get that the p-value for Ljung-Box statistical test is 0.01. Since p-value $\\approx$ 0.01 < 0.05 = $\\alpha$, we **reject our null hypothesis**. Thus, we conclude that our **residuals** are **not independently distributed** using significance level of 0.05. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "----\n",
    "\n",
    "Based on the hypothesis testing and the model summary above, we can conclude that the residual from our ARIMAX(4,1,0,'IDR/USD') is **not white noise**, based on significance level of 0.05. Thus, we **will** conduct GARCH to fit our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arimax_model = SARIMAX(bbca_info, \n",
    "                       order = (4,1,0),\n",
    "                       exog = exog_idr_usd.values).fit()\n",
    "\n",
    "arimax_resid = arimax_model.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis in the previous section, we know that **BBCA** stock price dataset have an **addition trend** and a **large entries/records**. As such, we will use **SARIMA** model to **train** our data instead of Holt or SES model. For simplicity, we will use a grid search to iteratively explore different combinations of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_grid_search(data, seasonal_period):\n",
    "    p = d = q = range(0, 2)\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], seasonal_period) for x in list(itertools.product(p, d, q))]\n",
    "    \n",
    "    mini = 1e6\n",
    "    \n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                mod = SARIMAX(data,\n",
    "                              order = param,\n",
    "                              seasonal_order = param_seasonal,\n",
    "                              enforce_stationarity = False,\n",
    "                              enforce_invertibility = False)\n",
    "\n",
    "                results = mod.fit()\n",
    "                \n",
    "                if results.aic < mini:\n",
    "                    mini = results.aic\n",
    "                    param_mini = param\n",
    "                    param_seasonal_mini = param_seasonal\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    print(f'The set of parameters with the minimum AIC is: SARIMA{param_mini}x{param_seasonal_mini} - AIC:{mini}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are observing by month, then pick **m = 12**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_grid_search(bbca_info, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary table above, we can see that the **SARIMA(1,1,1)x(0,1,1,12)** has the **lowest AIC**. Thus, we will try to analyze the performance of that model by using **diagnostic test**. If it is not suited to be a predictor model, we will pick another model (iteratively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_bbca = SARIMAX(bbca_info, \n",
    "                       order = (1,1,1),\n",
    "                       seasonal_order = (0,1,1,12),\n",
    "                       enforce_stationarity = False,\n",
    "                       enforce_invertibility = False)\n",
    "\n",
    "results_3_bbca = model_3_bbca.fit()\n",
    "\n",
    "results_3_bbca.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_resid = results_3_bbca.resid\n",
    "white_noise_sarima = acorr_ljungbox(sarima_resid ** 2, lags = [10], return_df = True)\n",
    "white_noise_sarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnostic Test & Conclusion**\n",
    "\n",
    "**Ljung-Box**\n",
    "\n",
    "----\n",
    "\n",
    "$H_0$ = Error term is white noise\n",
    "\n",
    "$H_1$ = Error term is not white noise\n",
    "\n",
    "Based on the table above, get that the p-value for Ljung-Box statistical test is ~0.05. Since p-value $\\approx$ 0.05 = $\\alpha$, we **fail to reject our null hypothesis**. Thus, we conclude that our **residuals** are **independently distributed** using significance level of 0.05. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "----\n",
    "\n",
    "Based on the hypothesis testing and the model summary above, we can conclude that the residual from our SARIMA(1,1,1)x(0,1,1,12) is **white noise**, based on significance level of 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model SARIMA(1,1,1)x(0,1,1,12)\n",
    "sarima_model = SARIMAX(bbca_info, \n",
    "                       order = (1,1,1),\n",
    "                       seasonal_order = (0,1,1,12),\n",
    "                       enforce_stationarity = False,\n",
    "                       enforce_invertibility = False).fit()\n",
    "\n",
    "sarima_resid = sarima_model.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GARCH Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a **financial data** which has **high volatility**, then it is a good idea to analyze and predict the future values using **GARCH model**. Here, GARCH is chosen instead of ARCH because in general, it has much **less parameters** and **performs better** than the ARCH model. The generalized autoregressive conditional heteroskedasticity (GARCH) model has only **three parameters** that allow for an **infinite number of squared roots** to influence the conditional variance.\n",
    "\n",
    "In addition to that, it was seen that our **previous models** have a sign of having a **GARCH effect**. Thus, we will test the performance of our previous models combined by the GARCH effect for both models (ARIMA and ARIMAX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA(4,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have noticed that ARIMA's residuals is not random from Ljung-Box test, we **can not conclude yet** that there exists **heteroskedasticity** effect. Thus, we will perform LM test to check it.\n",
    "\n",
    "$H_0$ ARCH Effect not exists\n",
    "\n",
    "$H_1$ARCH Effect exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_pvalue = het_arch(arima_resid, ddof = 4)[1]\n",
    "print(f'LM-test-Pvalue: {LM_pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we got that p-value $\\approx$ 0, then we **reject our null hypothesis**, and conclude that there exists a GARCH effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will **look** for the **p, q parameters** for the model **iteratively**, by observing the significancy of parameters of each possible values of p, q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_garch = arch_model(arima_resid, vol = 'GARCH', p = 1, q = 1)\n",
    "arima_garch = mdl_garch.fit()\n",
    "arima_garch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the only model with **significant parameters** is **GARCH(1,1)**. Thus, we will combine it with our base ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual Diagnosis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_std_resid = pd.Series(arima_garch.resid / arima_garch.conditional_volatility)\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "\n",
    "# Residual\n",
    "garch_std_resid.plot(ax = fig.add_subplot(3,1,1), title = 'GARCH Standardized-Residual', legend = False)\n",
    "\n",
    "# ACF/PACF\n",
    "plot_acf(garch_std_resid, zero = False, lags = 40, ax = fig.add_subplot(3,2,3))\n",
    "plot_pacf(garch_std_resid, zero = False, lags = 40, ax = fig.add_subplot(3,2,4))\n",
    "\n",
    "# QQ-Plot & Norm-Dist\n",
    "qqplot(garch_std_resid, line = 's', ax = fig.add_subplot(3,2,5)) \n",
    "plt.title(\"QQ Plot\")\n",
    "fig.add_subplot(3,2,6).hist(garch_std_resid, bins = 40)\n",
    "plt.title(\"Histogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise_garch = acorr_ljungbox(garch_std_resid, lags = [10], return_df=True)\n",
    "white_noise_garch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the residual diagnostic tests above, we can conclude that the residual follows the normal distribution and is a white noise. Therefore, no explanatory variable can be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMAX(4,1,0,'IDR/USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have noticed that ARIMA's residuals is not random from Ljung-Box test, we **can not conclude yet** that there exists **heteroskedasticity** effect. Thus, we will perform LM test to check it.\n",
    "\n",
    "$H_0$ ARCH Effect not exists\n",
    "\n",
    "$H_1$ARCH Effect exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_pvalue = het_arch(arimax_resid, ddof = 4)[1]\n",
    "print(f'LM-test-Pvalue: {LM_pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we got that p-value $\\approx$ 0.03 < 0.05 = $\\alpha$, then we **reject our null hypothesis**, and conclude that there exists a GARCH effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will **look** for the **p, q parameters** for the model **iteratively**, by observing the significancy of parameters of each possible values of p, q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_garchx = arch_model(arimax_resid, vol = 'GARCH', p = 1, q = 1)\n",
    "arimax_garch = mdl_garchx.fit()\n",
    "arimax_garch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the only model with **significant parameters** is **GARCH(1,1)**. Thus, we will combine it with our base ARIMAX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual Diagnosis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_std_resid = pd.Series(arimax_garch.resid / arimax_garch.conditional_volatility)\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "\n",
    "# Residual\n",
    "garch_std_resid.plot(ax = fig.add_subplot(3,1,1), title = 'GARCH Standardized-Residual', legend = False)\n",
    "\n",
    "# ACF/PACF\n",
    "plot_acf(garch_std_resid, zero = False, lags = 40, ax = fig.add_subplot(3,2,3))\n",
    "plot_pacf(garch_std_resid, zero = False, lags = 40, ax = fig.add_subplot(3,2,4))\n",
    "\n",
    "# QQ-Plot & Norm-Dist\n",
    "qqplot(garch_std_resid, line = 's', ax = fig.add_subplot(3,2,5)) \n",
    "plt.title(\"QQ Plot\")\n",
    "fig.add_subplot(3,2,6).hist(garch_std_resid, bins = 40)\n",
    "plt.title(\"Histogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise_garch = acorr_ljungbox(garch_std_resid, lags = [10], return_df=True)\n",
    "white_noise_garch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the residual diagnostic tests above, we can conclude that the residual follows the normal distribution and is a white noise. Therefore, no explanatory variable can be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-D Format Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the data into 3-D format so that it can be processed using CNN method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "    \n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 7\n",
    "\n",
    "X_bbca, y_bbca = split_sequence(bbca_info.values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform input from [samples, features] to [samples, timesteps, features]\n",
    "X_bbca = X_bbca.reshape((X_bbca.shape[0], X_bbca.shape[1], 1))\n",
    "print(\"BBCA shape : \", X_bbca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into two dataset\n",
    "    train, test = data[:int(0.75 * len(data))], data[int(0.75 * len(data)):]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bbca, X_test_bbca = split_dataset(X_bbca)\n",
    "y_train_bbca, y_test_bbca = split_dataset(y_bbca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BBCA Dataset\")\n",
    "print(\"X_train shape : \", X_train_bbca.shape)\n",
    "print(\"X_test shape : \", X_test_bbca.shape)\n",
    "print(\"y_train shape : \", y_train_bbca.shape)\n",
    "print(\"y_test shape : \", y_test_bbca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the TS data into Supervised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out = 7):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    \n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        \n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end < len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def build_model(dl_model, train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 100, 4\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\n",
    "    # define model\n",
    "    if dl_model == 'CNN':\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters = 16, \n",
    "                         kernel_size = 3, \n",
    "                         activation = 'relu',\n",
    "                         input_shape = (n_timesteps, n_features)))\n",
    "        model.add(MaxPooling1D(pool_size = 2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(10, \n",
    "                        activation = 'relu'))\n",
    "        model.add(Dense(n_outputs))\n",
    "        model.compile(loss = 'mse', \n",
    "                      optimizer = 'adam')\n",
    "\n",
    "        # fit network\n",
    "        model.fit(train_x, train_y, epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    elif dl_model == 'LSTM':\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, \n",
    "                       activation = 'relu', \n",
    "                       input_shape = (n_timesteps, n_features)))\n",
    "        model.add(Dense(100, \n",
    "                        activation = 'relu'))\n",
    "        model.add(Dense(n_outputs))\n",
    "        model.compile(loss = 'mse',\n",
    "                      optimizer = 'adam')\n",
    "\n",
    "        # fit network\n",
    "        model.fit(train_x, train_y, epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As CNN do not have an in-built forecasting function, we have to make it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = np.array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    \n",
    "    # reshape into [1, n_input, 1]\n",
    "    input_x = input_x.reshape((1, len(input_x), 1))\n",
    "    \n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "  \n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Forecasting\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    \n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "    \n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\n",
    "        # calculate rmse\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "  \n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    \n",
    "    score = np.sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "            \n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a single model\n",
    "def evaluate_model(dl_model, train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(dl_model, train, n_input)\n",
    "    \n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "  \n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "  \n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, \n",
    "                                 history,\n",
    "                                 n_input)\n",
    "    \n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "    \n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "  \n",
    "    # evaluate predictions days for each week\n",
    "    predictions = np.array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], \n",
    "                                       predictions)\n",
    "\n",
    "    return predictions, score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    \n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train, test = split_dataset(bbca_info.values)\n",
    "\n",
    "# Make into 3D\n",
    "train, dummy1 = split_sequence(train, 7)\n",
    "test, dummy2 = split_sequence(test, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model and get scores\n",
    "n_input = 7\n",
    "predictions_cnn, score_cnn, scores_cnn = evaluate_model('CNN', train, test, 7)\n",
    "\n",
    "# Summarize scores\n",
    "summarize_scores('CNN', \n",
    "                 score_cnn, \n",
    "                 scores_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model and get scores\n",
    "n_input = 7\n",
    "predictions_lstm, score_lstm, scores_lstm = evaluate_model('LSTM', train, test, 7)\n",
    "\n",
    "# Summarize scores\n",
    "summarize_scores('LSTM', \n",
    "                 score_lstm, \n",
    "                 scores_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'By using the CNN model, we get that the RMSE is {score_cnn}')\n",
    "print(f'By using the LSTM model, we get that the RMSE is {score_lstm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In this analytics, the prediction and forecasting will only performed on the 'conventional', which is the ARIMA, ARIMAX, and SARIMA model, and will not be performed on the GARCH model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA(4,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ARIMA mean forecast\n",
    "arima_pred = arima_model.get_prediction(start = '2020-04-02')\n",
    "arima_mean = arima_pred.predicted_mean\n",
    "\n",
    "# Get confidence intervals of ARIMA mean forecasts\n",
    "conf_int_arima = arima_pred.conf_int()\n",
    "\n",
    "# Plot mean ARIMA predictions and observed\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "plt.plot(arima_mean.index, \n",
    "         arima_mean, \n",
    "         label = 'ARIMA(4,1,0)')\n",
    "\n",
    "plt.fill_between(conf_int_arima.index, \n",
    "                 conf_int_arima['lower close'], \n",
    "                 conf_int_arima['upper close'], \n",
    "                 color = 'pink')\n",
    "\n",
    "plt.plot(bbca_info['2020-04-02':], \n",
    "         label = 'observed')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (IDR)')\n",
    "plt.title('ARIMA(4,1,0) on BBCA stock market \\'price\\' Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMAX(4,1,0,'IDR/USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMAX\n",
    "\n",
    "# Create ARIMAX mean forecast\n",
    "arimax_pred = arimax_model.get_prediction(start = '2020-04-02')\n",
    "arimax_mean = arimax_pred.predicted_mean\n",
    "\n",
    "# Get confidence intervals of ARIMAX mean forecasts\n",
    "conf_int_arimax = arimax_pred.conf_int()\n",
    "\n",
    "# Plot mean ARIMAX predictions and observed\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "plt.plot(arimax_mean.index, \n",
    "         arimax_mean, \n",
    "         label = 'ARIMAX(4,1,0,\\'IDR/USD\\')')\n",
    "\n",
    "plt.fill_between(conf_int_arimax.index, \n",
    "                 conf_int_arimax['lower close'], \n",
    "                 conf_int_arimax['upper close'],\n",
    "                 color = 'pink')\n",
    "\n",
    "plt.plot(bbca_info['2020-04-02':], \n",
    "         label = 'observed')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (IDR)')\n",
    "plt.title('ARIMAX(4,1,0,\\'IDR/USD\\') on BBCA stock market \\'price\\' Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA(1,1,1)x(0,1,1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA\n",
    "\n",
    "# Create SARIMA mean forecast\n",
    "sarima_pred = sarima_model.get_prediction(start = '2020-04-02')\n",
    "sarima_mean = sarima_pred.predicted_mean\n",
    "\n",
    "# Get confidence intervals of SARIMA mean forecasts\n",
    "conf_int_sarima = sarima_pred.conf_int()\n",
    "\n",
    "# Plot mean ARIMAX predictions and observed\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "plt.plot(sarima_mean.index, \n",
    "         sarima_mean, \n",
    "         label = 'SARIMA(1,1,1)x(0,1,1,12)')\n",
    "\n",
    "plt.fill_between(conf_int_sarima.index, \n",
    "                 conf_int_sarima['lower close'], \n",
    "                 conf_int_sarima['upper close'],\n",
    "                 color = 'pink')\n",
    "\n",
    "plt.plot(bbca_info['2020-04-02':], \n",
    "         label = 'observed')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (IDR)')\n",
    "plt.title('SARIMA(1,1,1)x(0,1,1,12) on BBCA stock market \\'price\\' Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA(4,1,0)-GARCH(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size = len(bbca_info.values)\n",
    "# arima_garch.forecast(horizon = test_size).variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMAX(4,1,0,'IDR/USD')-GARCH(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bbca = bbca_info['2020-04-02':]\n",
    "\n",
    "# Prediction for all models\n",
    "yhat_1_bbca = arima_model.get_prediction(start = '2020-04-02').predicted_mean\n",
    "yhat_2_bbca = arimax_model.get_prediction(start = '2020-04-02').predicted_mean\n",
    "yhat_3_bbca = sarima_model.get_prediction(start = '2020-04-02').predicted_mean\n",
    "\n",
    "# Print MSE\n",
    "print(\"MSE for ARIMA(4,1,0) prediction : \", mean_squared_error(y_bbca, yhat_1_bbca))\n",
    "print(\"MSE for ARIMAX(4,1,0,'IDR/USD') prediction : \", mean_squared_error(y_bbca, yhat_2_bbca))\n",
    "print(\"MSE for SARIMA(1,1,1)x(0,1,1,12) prediction : \", mean_squared_error(y_bbca, yhat_3_bbca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cetak Root Mean Square\n",
    "rmse_1 = np.sqrt(mean_squared_error(y_bbca, yhat_1_bbca))\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_bbca, yhat_2_bbca))\n",
    "rmse_3 = np.sqrt(mean_squared_error(y_bbca, yhat_3_bbca))\n",
    "rmse_4 = score_cnn\n",
    "rmse_5 = score_lstm\n",
    "\n",
    "rmse = [rmse_1, rmse_2, rmse_3, rmse_4, rmse_5]\n",
    "\n",
    "# Cetak Nama Model\n",
    "model = ['ARIMA(3,1,1)', 'ARIMAX(3,1,1,\\'IDR/USD\\')', 'SARIMA(1,1,1)x(0,1,1,12)', \n",
    "         'Convolutional Neural Network', 'Long Short-Term Memory']\n",
    "\n",
    "# Buat dictionary\n",
    "dict_data_bbca = {'Nama Model': model,\n",
    "                  'RMSE' : rmse}\n",
    "\n",
    "# Buat DataFramenya\n",
    "df_score_bbca = pd.DataFrame(dict_data_bbca).sort_values(by = 'RMSE', \n",
    "                                                         ascending = True).reset_index(drop = True)\n",
    "\n",
    "df_score_bbca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the AIC-BIC, Diagnostic Test, and also the RMSE results, we can conclude that:\n",
    "\n",
    "1. LSTM\n",
    "2. ARIMAX(4,1,0,'IDR/USD')\n",
    "3. ARIMA(4,1,0)\n",
    "4. CNN\n",
    "5. SARIMA(1,1,1)x(0,1,1,12)\n",
    "\n",
    "respectively, are the **best models** for our BBCA dataset since they are **statistically significance** and have the **lowest AIC-BIC** and **lowest RMSE**.\n",
    "\n",
    "We will see the forecasted result for each models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_time_series(data, model_result, n_steps, predicted_date, exog = None, exog_name = None, result = False):\n",
    "    arimax_pred = model_result.get_forecast(steps = n_steps,\n",
    "                                            # A week is chosen as forecasting for a long term \n",
    "                                            # is not wise and will certainly lead to a high bias/low confidence level\n",
    "                                            exog = exog_idr_usd[-1:-n_steps-1:-1], \n",
    "                                            dynamic = True)\n",
    "\n",
    "    arimax_mean = pd.DataFrame(arimax_pred.predicted_mean)\n",
    "    arimax_mean.index = predicted_date\n",
    "    arimax_mean.columns = ['close']\n",
    "\n",
    "    arimax_mean = pd.concat([data, arimax_mean], axis = 0)\n",
    "\n",
    "    # Get confidence intervals of ARIMAX(1,1,0,'IDR/USD') mean forecasts\n",
    "    conf_int_arimax = arimax_pred.conf_int()\n",
    "    conf_int_arimax.index = predicted_date\n",
    "\n",
    "\n",
    "    if result == False:\n",
    "        # Plot mean ARIMAX predictions and observed\n",
    "        plt.figure(figsize = (20,10))\n",
    "\n",
    "        plt.plot(arimax_mean[start_date:], \n",
    "                 label = f'Prediction ARIMAX(4,1,0,{exog_name}))', \n",
    "                 color = 'red')\n",
    "\n",
    "        plt.fill_between(conf_int_arimax.index, \n",
    "                         conf_int_arimax['lower close'], \n",
    "                         conf_int_arimax['upper close'], \n",
    "                         label = None,\n",
    "                         color = 'pink')\n",
    "\n",
    "        plt.plot(data[start_date:], \n",
    "                 label = 'observed', \n",
    "                 color = 'blue')\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price (IDR)')\n",
    "        plt.title(f'ARIMAX(4,1,0,{exog_name}) on BBCA stock market prediction \\'price\\' in the next 7 days')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        conf_int_arimax['close'] = arimax_mean['close'][-n_steps::1]\n",
    "        conf_int_arimax = conf_int_arimax[['lower close', 'close', 'upper close']]\n",
    "        return conf_int_arimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start observing date\n",
    "start_date = '2021-12-01'\n",
    "\n",
    "# Start predicted date\n",
    "x = '2023-03-01'\n",
    "predicted_dates = pd.date_range(start = dt.datetime.strptime(x, '%Y-%m-%d'), \n",
    "                                end = (dt.datetime.strptime(x, '%Y-%m-%d') + dt.timedelta(days = 6)).strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA(4,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forecast_time_series(data = bbca_info, \n",
    "                     model_result = arima_model, \n",
    "                     n_steps = 7, \n",
    "                     exog = None, \n",
    "                     exog_name = None, \n",
    "                     predicted_date = predicted_dates, \n",
    "                     result = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA(4,1,0) Prediction\n",
    "result_1 = forecast_time_series(data = bbca_info, \n",
    "                                model_result = arima_model, \n",
    "                                n_steps = 7, \n",
    "                                exog = None, \n",
    "                                exog_name = None, \n",
    "                                predicted_date = predicted_dates, \n",
    "                                result = True)\n",
    "\n",
    "result_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMAX(4,1,0,'IDR/USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time_series(data = bbca_info, \n",
    "                     model_result = arimax_model, \n",
    "                     n_steps = 7, \n",
    "                     exog = exog_idr_usd, \n",
    "                     exog_name = 'IDR/USD', \n",
    "                     predicted_date = predicted_dates, \n",
    "                     result = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMAX(4,1,0,'IDR/USD') Prediction\n",
    "result_2 = forecast_time_series(data = bbca_info, \n",
    "                                model_result = arimax_model, \n",
    "                                n_steps = 7, \n",
    "                                exog = exog_idr_usd, \n",
    "                                exog_name = 'IDR/USD', \n",
    "                                predicted_date = predicted_dates, \n",
    "                                result = True)\n",
    "\n",
    "result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA(1,1,1)x(0,1,1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time_series(data = bbca_info, \n",
    "                     model_result = sarima_model, \n",
    "                     n_steps = 7, \n",
    "                     exog = None, \n",
    "                     exog_name = None, \n",
    "                     predicted_date = predicted_dates, \n",
    "                     result = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA(1,1,1)x(0,1,1,12) Prediction\n",
    "result_3 = forecast_time_series(data = bbca_info, \n",
    "                                model_result = sarima_model, \n",
    "                                n_steps = 7, \n",
    "                                exog = None, \n",
    "                                exog_name = None, \n",
    "                                predicted_date = predicted_dates, \n",
    "                                result = True)\n",
    "\n",
    "result_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cnn[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short-Term Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lstm[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our model, we can see that for the **next 7 days**, we can expect that the **BBCA stock market price** will be relatively stagnant at 8790 IDR with a margin error of ~4.47%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical**\n",
    "\n",
    "---\n",
    "\n",
    "Consider the **ARIMA-GARCH** and **ARIMAX-GARCH** model that has been previously calculated. The model is **not used** here since the author still does not find a **valid resource** as to **combine** the result from ARIMA and GARCH model separately in Python, unlike **rugarch** in **R**.\n",
    "\n",
    "**Non-technical**\n",
    "\n",
    "---\n",
    "\n",
    "The BBCA stock market price do not show any sign of dropping anytime soon. So, if you are a **shareholder**, try **not to sell your stock**, except in case of emergencies. And for a **non-shareholder**, you probably want to **wait** when the price is showing a **decrement trend**, which is **probably** not in the near future (1-week time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the author is still an Undergraduate student who has only learn basic optimization method on portfolio, thus we will only conduct simple **Markowitz Frontier Model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = pd.DataFrame()\n",
    "\n",
    "portfolio.index = bbca_info.index\n",
    "\n",
    "# Get BBCA and ^JKSE close value\n",
    "portfolio['BBCA'] = bbca_info.values\n",
    "jkse_info = jkse_info['2020-04-01':]\n",
    "portfolio['^JKSE'] = jkse_info.values\n",
    "\n",
    "portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = portfolio / portfolio.shift(1)\n",
    "returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_portfolios = 10000\n",
    "weight = np.zeros((n_portfolios, 2))\n",
    "\n",
    "expectedReturn = np.zeros(n_portfolios)\n",
    "expectedVolatility = np.zeros(n_portfolios)\n",
    "sharpeRatio = np.zeros(n_portfolios)\n",
    "\n",
    "meanReturn = returns.mean()\n",
    "S = returns.cov()\n",
    "for k in range(n_portfolios):\n",
    "    # Generate random weights for each portfolio\n",
    "    w = np.array(np.random.random(2))\n",
    "    w = w / np.sum(w)\n",
    "    weight[k, :] = w\n",
    "    \n",
    "    # Expected return & volatility\n",
    "    expectedReturn[k] = np.sum(meanReturn * w)\n",
    "    expectedVolatility[k] = np.sqrt(np.matmul(w.T, np.matmul(S, w)))\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    sharpeRatio[k] = expectedReturn[k] / expectedVolatility[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIdx = sharpeRatio.argmax()\n",
    "optWeight = weight[maxIdx, :]\n",
    "\n",
    "maxIdx, optWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return vs Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "plt.scatter(expectedVolatility, expectedReturn, c = sharpeRatio)\n",
    "plt.scatter(expectedVolatility[maxIdx], expectedReturn[maxIdx])\n",
    "plt.xlabel('Expected Volatility')\n",
    "plt.ylabel('Expected Return')\n",
    "plt.title('Return vs Volatility')\n",
    "plt.colorbar(label = 'SR')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markowitz Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeSR(w):\n",
    "    w = np.array(w)\n",
    "    R = np.sum(meanReturn * w)\n",
    "    V = np.sqrt(np.matmul(w.T, np.matmul(S, w)))\n",
    "    \n",
    "    return -R / V\n",
    "\n",
    "def checkSumToOne(w):\n",
    "    return np.sum(w) - 1\n",
    "\n",
    "w0 = [0.5, 0.5]\n",
    "bounds = ((0, 1), (0, 1))\n",
    "constraints = ({'type': 'eq',\n",
    "                'fun': lambda x: np.sum(x) - 1})\n",
    "w_opt = minimize(negativeSR, \n",
    "                 w0, \n",
    "                 method = 'SLSQP', \n",
    "                 bounds = bounds,\n",
    "                 constraints = constraints)\n",
    "\n",
    "w_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizeVolatility(w):\n",
    "    w = np.array(w)\n",
    "    \n",
    "    return np.sqrt(np.matmul(w.T, np.matmul(S, w)))\n",
    "\n",
    "def getReturn(w):\n",
    "    w = np.array(w)\n",
    "    \n",
    "    return np.sum(meanReturn * w)\n",
    "\n",
    "w0 = [0.5, 0.5]\n",
    "bounds = ((0, 1), (0, 1))\n",
    "return_lst = np.linspace(min(expectedReturn), max(expectedReturn), 100)\n",
    "volatility_opt = []\n",
    "\n",
    "for R in return_lst:\n",
    "    constraints = ({'type': 'eq',\n",
    "                    'fun': lambda x: np.sum(x) - 1},\n",
    "                   {'type': 'eq',\n",
    "                    'fun': lambda w: getReturn(w) - R})\n",
    "    w_opt = minimize(minimizeVolatility, \n",
    "                     w0, \n",
    "                     method = 'SLSQP', \n",
    "                     bounds = bounds,\n",
    "                     constraints = constraints)\n",
    "    \n",
    "    volatility_opt.append(w_opt['fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientFrontierPlot():\n",
    "    plt.figure(figsize = (12, 8))\n",
    "\n",
    "    plt.scatter(expectedVolatility, expectedReturn, c = sharpeRatio)\n",
    "    plt.scatter(expectedVolatility[maxIdx], expectedReturn[maxIdx])\n",
    "    plt.plot(volatility_opt, return_lst)\n",
    "    plt.xlabel('Expected Volatility')\n",
    "    plt.ylabel('Expected Return')\n",
    "    plt.title('Return vs Volatility')\n",
    "    plt.colorbar(label = 'SR')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    optReturn = expectedReturn[maxIdx]\n",
    "    optVolatility = expectedVolatility[maxIdx]\n",
    "    optRatio = sharpeRatio[maxIdx]\n",
    "\n",
    "    print('Expected Annual Return', optReturn)\n",
    "    print('Annual Volatility', optVolatility)\n",
    "    print('Sharpe Ratio', optRatio)\n",
    "\n",
    "efficientFrontierPlot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Efficient Frontier is at the **left edge** of the curve, since it shows the **maximum return** we can get, with the **least possible risk**. The curve is also relatively **dense** because if we notice that BBCA.JK and ^JKSE almost have the **same trend**, which means that both have relatively same daily Expected Return. Thus, the author argue that there does not much difference between investing in BBCA.JK or ^JKSe stock market. But, this method suggests us to **invest heavily** in **^JKSE**, because as we can see that ^JKSE has **lower daily jump** between the prices, which means that its **less risky** for us to invest there, with relatively the **same return**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
